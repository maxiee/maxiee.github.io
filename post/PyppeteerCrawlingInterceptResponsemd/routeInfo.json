{"template":"/home/maxiee/Code/Blog/maxiee.github.io/src/pages/post","sharedHashesByProp":{},"data":{"post":{"name":"Pyppeteer Tutorial: Crawling by intercepting response","desc":"Pyppeteer is a Python-porting of Puppeteer. It's a perfect tool for web crawling. Most articles on pyppeteer crawling are focusing on extracting data from the rendered pages. In this article, I'll show you another approach: crawling by intercepting web requests from the page.","type":"md","link":"PyppeteerCrawlingInterceptResponsemd","create":"2018-12-30","filename":"PyppeteerCrawlingInterceptResponse.md"},"content":"## Intro\n\nPyppeteer is a Python-porting of Puppeteer. It's a perfect tool for web crawling. Most articles on pyppeteer crawling are focusing on extracting data from the rendered pages. In this article, I'll show you another approach: crawling by intercepting web requests from the page.\n\n## The Process of webpage rendering\n\nWhen we open a new webpage in browser, if it is a SPA (Single Page Application), the browser will first load the code of application framework. When the framework loaded, the page will make another request to fetch the data, and then rendering it on the page.\n\nThe whole process is as below:\n\n![img1](image/Pyppeteer1.png)\n\nIn many tutorials, they extracts data from the rendered page, using xpath, querySelector or BeautifulSoup. This approach is as below:\n\n![img1](image/Pyppeteer2.png)\n\nIn this blog, we extract data by intercepting requests, which is as below:\n\n![img1](image/Pyppeteer3.png)\n\nThe data is usually formatted in JSON, which is easy to parse.\n\n## Pyppeteer Page Class\n\nIn pyppeter, a page class is corresponding to a page in the browser.\n\nWe will hook on the reponse event from EventEmitter:\n\n```python\nasync def interceptResponse(response):\n    # your parsing logic\n    ...\n\nbrowser = await launch(headless=False, devtools=True)\n\npage = await browser.newPage()\n\npage.on('response', \n        lambda response: asyncio.ensure_future(interceptResponse(response)))\n```\n\nThe type of response is [pyppeteer.network_manager.Response](https://miyakogi.github.io/pyppeteer/reference.html#response-class). It has many usefull fields:\n\n| Field   | Type    | Comment                                                      |\n| ------- | ------- | ------------------------------------------------------------ |\n| headers | dict    | dictionary of HTTP headers of this response                  |\n| json    | dict    | Get JSON representation of response body.                    |\n| ok      | bool    | Return bool whether this request is successful (200-299) or not. |\n| request | Request | Get matching Request object.                                 |\n| status  | int     | Status code of the response.                                 |\n| text    | str     | Get text representation of response body.                    |\n| url     | str     | URL of the response.                                         |\n\nAmong abrove:\n\n-   json is data we need, in type of Python dict!! We can use it directly!!\n-   we can use url to tell filter the response we need\n\nA example of interceptResponse may as below:\n\n```python\ndef url_feed_parse_filter(response):\n    json_data = response.data\n    # parse logic for feed response\n    # ...\n\nasync def interceptResponse(response):\n    url = response.url\n    if not response.ok:\n        print('request %s failed' % url)\n        return\n\n    if url == URL_FILTER_FEED:\n        url_feed_parse_filter(response)\n```\n\n## Conclusion\n\nIn this blog, I introduce a approach of crawling data by intercepting response.\n\nThe advantage of this approach is that instead of  extract data from rendered page, we directly parse the API response JSON, it's like we continue developing the application."},"path":"post/PyppeteerCrawlingInterceptResponsemd"}
